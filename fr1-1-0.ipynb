{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":105399,"databundleVersionId":12733338,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- 1. Setup & Imports ---\nimport polars as pl\nimport numpy as np\nimport lightgbm as lgb\nimport xgboost as xgb\nimport optuna\nimport gc\nfrom pathlib import Path\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.sparse import csr_matrix\n\nprint(\"--- Ultimate Flight Recommender: Clean Paths Version ---\")\n\nRANDOM_STATE = 42\nN_COMPONENTS = 15\nOPTUNA_TRIALS = 25\nnp.random.seed(RANDOM_STATE)\nINPUT_DIR = Path(\"/kaggle/input/aeroclub-recsys-2025\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:58:55.281471Z","iopub.execute_input":"2025-08-07T20:58:55.281784Z","iopub.status.idle":"2025-08-07T20:58:55.287565Z","shell.execute_reply.started":"2025-08-07T20:58:55.281759Z","shell.execute_reply":"2025-08-07T20:58:55.286652Z"}},"outputs":[{"name":"stdout","text":"--- Ultimate Flight Recommender: Clean Paths Version ---\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def hitrate_at_3(y_true, y_pred, groups):\n    df = pl.DataFrame({'group': groups, 'pred': y_pred, 'true': y_true})\n    return df.sort([\"group\", \"pred\"], descending=[False, True]).group_by(\"group\", maintain_order=True).head(3).group_by(\"group\").agg(pl.col(\"true\").max()).select(pl.col(\"true\").mean()).item()\n\ndef re_rank(test: pl.DataFrame, submission: pl.DataFrame, penalty_factor=0.1):\n    COLS_TO_COMPARE = [\"legs0_departureAt\", \"legs0_arrivalAt\", \"legs1_departureAt\", \"legs1_arrivalAt\", \"legs0_segments0_flightNumber\", \"legs1_segments0_flightNumber\"]\n    test = test.with_columns([pl.col(c).cast(str).fill_null(\"NULL\") for c in COLS_TO_COMPARE if c in test.columns])\n    df = submission.join(test, on=[\"Id\", \"ranker_id\"], how=\"left\")\n    df = df.with_columns(pl.concat_str([c for c in COLS_TO_COMPARE if c in df.columns]).alias(\"flight_hash\"))\n    df = df.with_columns(pl.max(\"pred_score\").over([\"ranker_id\", \"flight_hash\"]).alias(\"max_score_same_flight\"))\n    df = df.with_columns((pl.col(\"pred_score\") - penalty_factor * (pl.col(\"max_score_same_flight\") - pl.col(\"pred_score\"))).alias(\"reorder_score\"))\n    df = df.with_columns(pl.col(\"reorder_score\").rank(method=\"ordinal\", descending=True).over(\"ranker_id\").cast(pl.Int32).alias(\"new_selected\"))\n    return df.select([\"Id\", \"ranker_id\", \"new_selected\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:58:58.354486Z","iopub.execute_input":"2025-08-07T20:58:58.354770Z","iopub.status.idle":"2025-08-07T20:58:58.361646Z","shell.execute_reply.started":"2025-08-07T20:58:58.354745Z","shell.execute_reply":"2025-08-07T20:58:58.360864Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_raw = pl.read_parquet(INPUT_DIR / 'train.parquet')\ntest_raw = pl.read_parquet(INPUT_DIR / 'test.parquet')\ntrain_height = train_raw.height\n\ndata = pl.concat([\n    train_raw,\n    test_raw.with_columns(pl.lit(0, dtype=pl.Int64).alias(\"selected\"))\n], how=\"vertical_relaxed\")\ndel test_raw\ngc.collect()\n\n# Duration parsing\ndef dur_to_min(col):\n    days = col.str.extract(r\"^(\\d+)\\.\", 1).cast(pl.Int64).fill_null(0) * 1440\n    time_str = pl.when(col.str.contains(r\"^\\d+\\.\")).then(col.str.replace(r\"^\\d+\\.\", \"\")).otherwise(col)\n    hours = time_str.str.extract(r\"^(\\d+):\", 1).cast(pl.Int64).fill_null(0) * 60\n    minutes = time_str.str.extract(r\":(\\d+):\", 1).cast(pl.Int64).fill_null(0)\n    return (days + hours + minutes).fill_null(0)\n\ndf = data.clone()\ndur_cols = [c for c in df.columns if \"duration\" in c and df[c].dtype == pl.Utf8]\ndf = df.with_columns([dur_to_min(pl.col(c)).alias(c) for c in dur_cols])\n\ndf = df.with_columns([\n    (pl.col(\"legs0_duration\").fill_null(0) + pl.col(\"legs1_duration\").fill_null(0)).alias(\"total_duration\"),\n    pl.col(\"Id\").count().over(\"ranker_id\").alias(\"group_size\"),\n    pl.sum_horizontal(pl.col(f\"legs0_segments{i}_duration\").is_not_null() for i in range(4)).alias(\"n_segments_leg0\"),\n    pl.sum_horizontal(pl.col(f\"legs1_segments{i}_duration\").is_not_null() for i in range(4)).alias(\"n_segments_leg1\"),\n    (pl.col(\"totalPrice\").rank(\"average\").over(\"ranker_id\") / pl.col(\"group_size\")).alias(\"price_pct_rank\"),\n    (pl.col(\"totalPrice\") == pl.col(\"totalPrice\").min().over(\"ranker_id\")).alias(\"is_cheapest\"),\n    (pl.col(\"total_duration\") == pl.col(\"total_duration\").min().over(\"ranker_id\")).alias(\"is_fastest\"),\n])\ndf = df.with_columns(\n    (pl.col(\"n_segments_leg0\") + pl.col(\"n_segments_leg1\")).alias(\"total_segments\")\n)\ndf = df.with_columns([\n    (pl.col(\"total_segments\") == pl.col(\"total_segments\").min().over(\"ranker_id\")).alias(\"is_min_segments\"),\n])\n\ndf = df.join(\n    train_raw.group_by('legs0_segments0_marketingCarrier_code').agg(pl.mean('selected').alias('carrier0_pop')),\n    on='legs0_segments0_marketingCarrier_code', how='left'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:59:01.534800Z","iopub.execute_input":"2025-08-07T20:59:01.535431Z","execution_failed":"2025-08-07T21:01:42.548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"svd_source_df = train_raw.select([\"profileId\", \"legs0_segments0_marketingCarrier_code\"])\nuser_map = svd_source_df.select(\"profileId\").unique().with_row_index(\"user_code\")\nitem_map = svd_source_df.select(\"legs0_segments0_marketingCarrier_code\").unique().with_row_index(\"item_code\")\nsvd_source_df = svd_source_df.join(user_map, on=\"profileId\").join(item_map, on=\"legs0_segments0_marketingCarrier_code\")\n\nsparse_matrix = csr_matrix((np.ones(len(svd_source_df)), (svd_source_df['user_code'], svd_source_df['item_code'])), shape=(len(user_map), len(item_map)))\nsvd = TruncatedSVD(n_components=N_COMPONENTS, random_state=RANDOM_STATE)\nuser_embeddings = svd.fit_transform(sparse_matrix)\nitem_embeddings = svd.components_.T\n\nuser_svd_df = pl.DataFrame(user_embeddings, schema=[f\"user_svd_{i}\" for i in range(N_COMPONENTS)]).with_columns(user_map.get_column(\"profileId\"))\nitem_svd_df = pl.DataFrame(item_embeddings, schema=[f\"item_svd_{i}\" for i in range(N_COMPONENTS)]).with_columns(item_map.get_column(\"legs0_segments0_marketingCarrier_code\"))\n\ndf = df.join(user_svd_df, on=\"profileId\", how=\"left\")\ndf = df.join(item_svd_df, on=\"legs0_segments0_marketingCarrier_code\", how=\"left\")\ndata = df.fill_null(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:57:53.923935Z","iopub.status.idle":"2025-08-07T20:57:53.924169Z","shell.execute_reply.started":"2025-08-07T20:57:53.924063Z","shell.execute_reply":"2025-08-07T20:57:53.924073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 5: Model Training & Submission ---\nprint(\"\\nðŸ“¦ Preparing data for modeling...\")\n\ntrain_df = data.head(train_height)\ntest_df = data.tail(len(data) - train_height)\nfeature_cols = [col for col in data.columns if col not in [\n    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',\n    'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt'\n] and data[col].dtype.is_numeric()]\n\n# -------------------------------\n# Optuna Hyperparameter Tuning (XGBoost)\n# -------------------------------\nprint(\"\\nðŸŽ¯ Starting Optuna tuning for XGBoost...\")\n\nX_all = train_df.select(feature_cols)\ny_all = train_df.select('selected')\ngroups_all = train_df.select('ranker_id')\n\n# Time-based split (or random)\nsplit_point = int(0.8 * train_height)\nX_tr, X_val = X_all[:split_point], X_all[split_point:]\ny_tr, y_val = y_all[:split_point], y_all[split_point:]\ngroups_tr = groups_all[:split_point]\ngroups_val = groups_all[split_point:]\n\ngroup_sizes_tr = groups_tr.group_by('ranker_id', maintain_order=True).agg(pl.len())['len'].to_numpy()\ngroup_sizes_val = groups_val.group_by('ranker_id', maintain_order=True).agg(pl.len())['len'].to_numpy()\n\ndtrain = xgb.DMatrix(X_tr.to_pandas(), label=y_tr.to_pandas(), group=group_sizes_tr)\ndval = xgb.DMatrix(X_val.to_pandas(), label=y_val.to_pandas(), group=group_sizes_val)\n\ndef objective(trial):\n    params = {\n        'objective': 'rank:pairwise',\n        'eval_metric': 'ndcg@3',\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 6, 12),\n        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n        'lambda': trial.suggest_float('lambda', 1e-2, 10.0, log=True),\n        'alpha': trial.suggest_float('alpha', 1e-2, 10.0, log=True),\n        'seed': RANDOM_STATE\n    }\n    model = xgb.train(\n        params, dtrain, num_boost_round=1000,\n        evals=[(dval, \"val\")], early_stopping_rounds=50,\n        verbose_eval=False\n    )\n    return model.best_score\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=OPTUNA_TRIALS)\n\nbest_xgb_params = study.best_params\nbest_iteration = int(study.best_trial.user_attrs.get('best_iteration', 300) * 1.2)\nprint(f\"ðŸ”§ Best XGBoost NDCG@3: {study.best_value:.5f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:57:53.925169Z","iopub.status.idle":"2025-08-07T20:57:53.925397Z","shell.execute_reply.started":"2025-08-07T20:57:53.925298Z","shell.execute_reply":"2025-08-07T20:57:53.925307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nðŸš€ Training final models...\")\n\n# Full training\nX_train = train_df.select(feature_cols).to_pandas()\ny_train = train_df.select('selected').to_pandas()\ngroup_sizes_train = train_df.select('ranker_id').to_pandas()['ranker_id'].value_counts(sort=False).to_numpy()\n\nX_test = test_df.select(feature_cols).to_pandas()\n\n# Scale for LR\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# --- Model 1: XGBoost ---\ndtrain_full = xgb.DMatrix(X_train, label=y_train, group=group_sizes_train)\ndtest = xgb.DMatrix(X_test)\n\nfinal_xgb = xgb.train(\n    {'objective': 'rank:pairwise', 'eval_metric': 'ndcg@3', 'seed': RANDOM_STATE, **best_xgb_params},\n    dtrain_full,\n    num_boost_round=best_iteration\n)\npreds_xgb = final_xgb.predict(dtest)\n\n# --- Model 2: LightGBM ---\nlgb_train = lgb.Dataset(X_train, label=y_train, group=group_sizes_train)\nlgb_params = {\n    'objective': 'lambdarank',\n    'metric': 'ndcg',\n    'eval_at': [3],\n    'verbose': -1,\n    'seed': RANDOM_STATE\n}\nfinal_lgbm = lgb.train(lgb_params, lgb_train, num_boost_round=1200)\npreds_lgbm = final_lgbm.predict(X_test)\n\n# --- Model 3: Logistic Regression ---\nlr_model = LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced', max_iter=1000, C=0.01)\nlr_model.fit(X_train_scaled, y_train.values.ravel())\npreds_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:57:53.926043Z","iopub.status.idle":"2025-08-07T20:57:53.926326Z","shell.execute_reply.started":"2025-08-07T20:57:53.926213Z","shell.execute_reply":"2025-08-07T20:57:53.926225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nðŸ“¤ Creating submission...\")\n\n# Ensemble\nensemble_preds = 0.45 * preds_xgb + 0.45 * preds_lgbm + 0.10 * preds_lr\n\n# Re-rank and output\nraw_test_df = pl.read_parquet(INPUT_DIR / 'test.parquet')\nsub_df = test_df.select(['Id', 'ranker_id']).with_columns(pl.Series('pred_score', ensemble_preds))\nfinal_submission = re_rank(raw_test_df, sub_df).rename({\"new_selected\": \"selected\"})\n\n# Check and save\nassert final_submission.shape[0] == raw_test_df.shape[0], \"Row count mismatch!\"\nfinal_submission.write_csv(\"submission.csv\")\nprint(\"âœ… Final 'submission.csv' generated!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:57:53.927105Z","iopub.status.idle":"2025-08-07T20:57:53.927319Z","shell.execute_reply.started":"2025-08-07T20:57:53.927221Z","shell.execute_reply":"2025-08-07T20:57:53.927231Z"}},"outputs":[],"execution_count":null}]}